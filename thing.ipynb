{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfc7c21a",
   "metadata": {},
   "source": [
    "# Terraria wiki article recommender\n",
    "### Hubert Nowakowski 160302\n",
    "### Mukhammad Sattorov 159351"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c34dfa",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Introduction\n",
    "This project recommends similar articles from the terraria wiki based on text contents of said articles. Our dataset is comprised of a total of 2000 different pages scraped off of `terraria.wiki.gg`. We have chosen this wiki in particular for 3 main reasons:\n",
    "1. The person writing this likes the game and knows it very well :^)\n",
    "2. The amount of articles on this wiki is a little under 6 thousand, so we could get decent coverage with a sane amount of articles downloaded.\n",
    "3. A lot of mechanics, NPC's, items, events, bosses, etc. in this game affect and depend each other, so we were hoping to see some interesting relations between articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e4e702",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe8803e",
   "metadata": {},
   "source": [
    "### Approach and libraries used\n",
    "We have taken the approach of starting on the main page of the wiki, collecting all the hyperlinks leading to other pages into a queue, and performing bfs. On every page we visited from that point onwards, we took all of the text from the main article div, as well as added all the non-visted hyperlinks into the queue for further search.\n",
    "\n",
    "In an ideal world we'd have been able to just blast requests as fast as possible and be done with downloading the dataset within a few minutes at most. The scraping guidelines of the wiki, however, have specified a 1 second cooldown between requests. This still wouldn't have been too bad, but even a 1 second cooldown was short enough to trigger cloudflare's captchas and eventually put the scraper's ip on a blacklist for a few hours.\n",
    "\n",
    "This forced us to ditch the regular requests library in favour of cloudscraper, which works in a nigh-identical way, but employs some protections and workarounds for cloudflare's protections specifically. After a bit of trial and error with request cooldowns and headers, we have arrived at a working solution visible below. This implementation allowed us to scrape aforementioned 2000 articles within a *servicable* timeframe of a little over 2 hours.\n",
    "\n",
    "Aside from some hurdles with overcoming anti-bot protection, the remainder of scraping the data was very straightforward - a very standardised layout of the wiki's pages allowed us to just inspect a given div on each article for both text content and other links.\n",
    "\n",
    "The downloaded dataset has been put into a `raw_text.csv` file for further processing explained in the next section.\n",
    "### The code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad994879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import re\n",
    "from time import sleep\n",
    "from random import random\n",
    "from random import uniform\n",
    "import cloudscraper\n",
    "import pandas as pd\n",
    "\n",
    "# scraping guidelines\n",
    "# https://terraria.wiki.gg/robots.txt\n",
    "\n",
    "URL = \"https://terraria.wiki.gg\"\n",
    "\n",
    "def get_url(url, scraper):\n",
    "    res = scraper.get(url)\n",
    "    if res.status_code != 200:\n",
    "        print(f\"WARNING! Get request returned code other than 200: {res.status_code}\")\n",
    "    if res.status_code == 429:\n",
    "        print(\"Returned 429, retrying in 15 seconds\")\n",
    "        sleep(15)\n",
    "        res = scraper.get(url)\n",
    "        if res.status_code == 429:\n",
    "            raise ConnectionRefusedError(\"try in a few hours or on a phone hotspot lol\")\n",
    "    if res.status_code == 200:\n",
    "        print(f\"Url fetched successfully: {url}\")\n",
    "    return res.text\n",
    "\n",
    "def bfs(initial_links, scraper, df, iterations):\n",
    "    queue = initial_links\n",
    "    visited = set(initial_links[:])\n",
    "    while iterations and queue:\n",
    "        sleep(uniform(2, 4) + 1) # robots.txt says 1 sec cooldown is fine but i still sometimes got blocked by cloudflare\n",
    "        curr_link = queue.pop(0)\n",
    "        curr = get_url(URL + curr_link, scraper)\n",
    "        soup = bs4.BeautifulSoup(curr, \"html.parser\")\n",
    "        title = soup.find(\"h1\", {\"id\": \"firstHeading\"}).text\n",
    "        body = soup.find(\"div\", {\"class\": \"mw-content-ltr mw-parser-output\"})\n",
    "        new_links = [a.get(\"href\") for a in body.find_all(\"a\", attrs={'href': re.compile(r'^/wiki')}) if not a.find(\"img\")]\n",
    "        df = pd.concat((df,\n",
    "                         pd.DataFrame([[title, curr_link, body.text.strip().replace('\\n', ' ').replace('  ', ' ')]],\n",
    "                                       columns=[\"title\", \"link\", \"body\"])), ignore_index=True)\n",
    "        for link in new_links:\n",
    "            if link not in visited:\n",
    "                visited.add(link)\n",
    "                queue.append(link)\n",
    "        iterations -= 1\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def scrape():\n",
    "    df = pd.DataFrame(columns=[\"title\", \"link\", \"body\"])\n",
    "    scraper = cloudscraper.create_scraper(delay=10, browser={'custom': 'ScraperBot/1.0',})\n",
    "    main_site = get_url(URL, scraper)\n",
    "    # id=\"main-section\" <-- extract all wiki.gg hrefs from here and start bfs\n",
    "    # h1 id=\"firstHeading\" (page title) div class=\"mw-content-ltr mw-parser-output\" (body) <--- for other pages\n",
    "    soup = bs4.BeautifulSoup(main_site, \"html.parser\")\n",
    "    body = soup.find(\"div\", {\"id\": \"main-section\"})\n",
    "    links = [a.get(\"href\") for a in body.find_all(\"a\", attrs={'href': re.compile(r'^/wiki')}) if not a.find(\"img\")]\n",
    "    df = bfs(links, scraper, df, 2000)\n",
    "    #print(df)\n",
    "    df.to_csv(\"raw_text.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63b6320",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Stemming and lemmatization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
