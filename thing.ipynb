{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfc7c21a",
   "metadata": {},
   "source": [
    "# Terraria wiki article recommender\n",
    "### Hubert Nowakowski 160302\n",
    "### Mukhammad Sattorov 159351"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c34dfa",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Introduction\n",
    "This project recommends similar articles from the terraria wiki based on text contents of said articles. Our dataset is comprised of a total of 1000 different pages scraped off of `terraria.wiki.gg`. We have chosen this wiki in particular for 3 main reasons:\n",
    "1. The person writing this likes the game and knows it very well :^)\n",
    "2. The amount of articles on this wiki is a little under 6 thousand, so we could get decent coverage with a sane amount of articles downloaded.\n",
    "3. A lot of mechanics, NPC's, items, events, bosses, etc. in this game affect and depend each other, so we were hoping to see some interesting relations between articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e4e702",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe8803e",
   "metadata": {},
   "source": [
    "### Approach and libraries used\n",
    "We have taken the approach of starting on the main page of the wiki, collecting all the hyperlinks leading to other pages into a queue, and performing bfs. On every page we visited from that point onwards, we took all of the text from the main article div, as well as added all the non-visted hyperlinks into the queue for further search.\n",
    "\n",
    "In an ideal world we'd have been able to just blast requests as fast as possible and be done with downloading the dataset within a few minutes at most. The scraping guidelines of the wiki, however, have specified a 1 second cooldown between requests. This still wouldn't have been too bad, but even a 1 second cooldown was short enough to trigger cloudflare's captchas and eventually put the scraper's ip on a blacklist for a few hours.\n",
    "\n",
    "This forced us to ditch the regular requests library in favour of cloudscraper, which works in a nigh-identical way, but employs some protections and workarounds for cloudflare's protections specifically. After a bit of trial and error with request cooldowns and headers, we have arrived at a working solution visible below. This implementation allowed us to scrape aforementioned 1000 articles within a *servicable* timeframe of a little over 40 minutes.\n",
    "\n",
    "Aside from some hurdles with overcoming anti-bot protection, the remainder of scraping the data was very straightforward - a very standardised layout of the wiki's pages allowed us to just inspect a given div on each article for both text content and other links.\n",
    "\n",
    "The downloaded dataset has been put into a `raw_text.csv` file for further processing explained in the next section.\n",
    "### The code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad994879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import re\n",
    "from time import sleep\n",
    "from random import uniform\n",
    "import cloudscraper\n",
    "import pandas as pd\n",
    "\n",
    "# scraping guidelines\n",
    "# https://terraria.wiki.gg/robots.txt\n",
    "\n",
    "URL = \"https://terraria.wiki.gg\"\n",
    "\n",
    "def clean_link(link):\n",
    "    link = link.split('#')[0]\n",
    "    link = link.rstrip('/')\n",
    "    return re.sub(r'\\?.*$', '', link)\n",
    "\n",
    "\n",
    "def get_url(url, scraper):\n",
    "    res = scraper.get(url)\n",
    "    if res.status_code != 200:\n",
    "        print(f\"WARNING! Get request returned code other than 200: {res.status_code}\")\n",
    "    if res.status_code == 429:\n",
    "        print(\"Returned 429, retrying in 15 seconds\")\n",
    "        sleep(15)\n",
    "        res = scraper.get(url)\n",
    "        if res.status_code == 429:\n",
    "            raise ConnectionRefusedError(\"try in a few hours or on a phone hotspot lol\")\n",
    "    if res.status_code == 200:\n",
    "        print(f\"Url fetched successfully: {url}\")\n",
    "    return res.text\n",
    "\n",
    "def bfs(initial_links, scraper, df, iterations):\n",
    "    queue = initial_links\n",
    "    visited = set(initial_links[:])\n",
    "    while iterations and queue:\n",
    "        sleep(uniform(1, 2) + 1) # robots.txt says 1 sec cooldown is fine but i still sometimes got blocked by cloudflare\n",
    "        curr_link = clean_link(queue.pop(0))\n",
    "        curr = get_url(URL + curr_link, scraper)\n",
    "        soup = bs4.BeautifulSoup(curr, \"html.parser\")\n",
    "        title = soup.find(\"h1\", {\"id\": \"firstHeading\"}).text\n",
    "        body = soup.find(\"div\", {\"class\": \"mw-content-ltr mw-parser-output\"})\n",
    "        new_links = [a.get(\"href\") for a in body.find_all(\"a\", attrs={'href': re.compile(r'^/wiki')}) if not a.find(\"img\")]\n",
    "        df = pd.concat((df,\n",
    "                         pd.DataFrame([[title, curr_link, body.text.strip().replace('\\n', ' ').replace('  ', ' ')]],\n",
    "                                       columns=[\"title\", \"link\", \"body\"])), ignore_index=True)\n",
    "        for link in new_links:\n",
    "            if link not in visited:\n",
    "                visited.add(link)\n",
    "                queue.append(link)\n",
    "        iterations -= 1\n",
    "        print(f\"Remaining: {iterations}\")\n",
    "    return df.drop_duplicates(subset='link', keep='first')\n",
    "\n",
    "\n",
    "# entrypoint for this entire section\n",
    "def scrape():\n",
    "    df = pd.DataFrame(columns=[\"title\", \"link\", \"body\"])\n",
    "    scraper = cloudscraper.create_scraper(delay=10, browser={'custom': 'ScraperBot/1.0',})\n",
    "    main_site = get_url(URL, scraper)\n",
    "    # id=\"main-section\" <-- extract all wiki.gg hrefs from here and start bfs\n",
    "    # h1 id=\"firstHeading\" (page title) div class=\"mw-content-ltr mw-parser-output\" (body) <--- for other pages\n",
    "    soup = bs4.BeautifulSoup(main_site, \"html.parser\")\n",
    "    body = soup.find(\"div\", {\"id\": \"main-section\"})\n",
    "    links = [a.get(\"href\") for a in body.find_all(\"a\", attrs={'href': re.compile(r'^/wiki')}) if not a.find(\"img\")]\n",
    "    df = bfs(links, scraper, df, 1000)\n",
    "    #print(df)\n",
    "    df.to_csv(\"raw_text.csv\")\n",
    "\n",
    "#scrape()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63b6320",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc90d3b2",
   "metadata": {},
   "source": [
    "### Approach\n",
    "TODO\n",
    "\n",
    "### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b963fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example processed text: terraria author re-logicdr studiosengine software not to be confuse with the latin plural of terrari\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "def tokenize(df):\n",
    "    df['tokens'] = df['body'].map(word_tokenize)\n",
    "    return df\n",
    "\n",
    "def lemmatize(df):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df['lemma'] = df['tokens'].map(lambda tokens: [lemmatizer.lemmatize(token.lower(), pos='v') for token in tokens])\n",
    "    df['lemma'] = df['lemma'].map(lambda x: ' '.join(x))\n",
    "    return df\n",
    "\n",
    "# entrypoint for this entire section\n",
    "def process_text():\n",
    "    df = pd.read_csv('raw_text.csv', index_col=0).fillna(' ')\n",
    "    df = lemmatize(tokenize(df))\n",
    "    df = df.drop(columns=['tokens', 'body'])\n",
    "    df.to_csv(\"processed_text.csv\")\n",
    "    return df\n",
    "\n",
    "df = process_text()\n",
    "print(f\"Example processed text: {df['lemma'][0][:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9438d09",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Similarities\n",
    "### Approach\n",
    "TODO\n",
    "\n",
    "### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af025a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000000000</th>\n",
       "      <th>000005162020</th>\n",
       "      <th>00012</th>\n",
       "      <th>00016</th>\n",
       "      <th>0002</th>\n",
       "      <th>00021</th>\n",
       "      <th>...</th>\n",
       "      <th>„ÇÄ„Çâ„Åï„Åç„ÅÆ„ÅÑ„Å®</th>\n",
       "      <th>„ÇÜ„ÅÜ„Åó„ÇÉ„ÅÆË°£Ë£Ö</th>\n",
       "      <th>„Éé„Éº„Éà</th>\n",
       "      <th>ÂãáÊ∞ó</th>\n",
       "      <th>Ê≤≥Á´•</th>\n",
       "      <th>ÁßªÂä®‰∏≠ÂõΩÁâà</th>\n",
       "      <th>ÁÆÄ‰Ωì‰∏≠Êñá</th>\n",
       "      <th>ùêºùëöùëùùëíùëõùëëùëñùëõùëî</th>\n",
       "      <th>ùëéùëùùëùùëüùëúùëéùëê‚Ñéùëíùë†</th>\n",
       "      <th>ùëëùëúùëúùëö</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Terraria</td>\n",
       "      <td>/wiki/Terraria</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ores</td>\n",
       "      <td>/wiki/Ore</td>\n",
       "      <td>0.029576</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recipes</td>\n",
       "      <td>/wiki/Recipes</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Weapons</td>\n",
       "      <td>/wiki/Weapons</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Magic weapons</td>\n",
       "      <td>/wiki/Magic</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Frozen Slime Block</td>\n",
       "      <td>/wiki/Frozen_Slime_Block</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Vortex Fragment Block</td>\n",
       "      <td>/wiki/Vortex_Fragment_Block</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Nebula Fragment Block</td>\n",
       "      <td>/wiki/Nebula_Fragment_Block</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Stardust Fragment Block</td>\n",
       "      <td>/wiki/Stardust_Fragment_Block</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Solar Fragment Block</td>\n",
       "      <td>/wiki/Solar_Fragment_Block</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>976 rows √ó 32303 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       title                           link        00  000  \\\n",
       "0                   Terraria                 /wiki/Terraria  0.000000  0.0   \n",
       "1                       Ores                      /wiki/Ore  0.029576  0.0   \n",
       "2                    Recipes                  /wiki/Recipes  0.000000  0.0   \n",
       "3                    Weapons                  /wiki/Weapons  0.000000  0.0   \n",
       "4              Magic weapons                    /wiki/Magic  0.000000  0.0   \n",
       "..                       ...                            ...       ...  ...   \n",
       "995       Frozen Slime Block       /wiki/Frozen_Slime_Block  0.000000  0.0   \n",
       "996    Vortex Fragment Block    /wiki/Vortex_Fragment_Block  0.000000  0.0   \n",
       "997    Nebula Fragment Block    /wiki/Nebula_Fragment_Block  0.000000  0.0   \n",
       "998  Stardust Fragment Block  /wiki/Stardust_Fragment_Block  0.000000  0.0   \n",
       "999     Solar Fragment Block     /wiki/Solar_Fragment_Block  0.000000  0.0   \n",
       "\n",
       "     0000000000  000005162020  00012  00016  0002  00021  ...  „ÇÄ„Çâ„Åï„Åç„ÅÆ„ÅÑ„Å®  \\\n",
       "0           0.0           0.0    0.0    0.0   0.0    0.0  ...      0.0   \n",
       "1           0.0           0.0    0.0    0.0   0.0    0.0  ...      0.0   \n",
       "2           0.0           0.0    0.0    0.0   0.0    0.0  ...      0.0   \n",
       "3           0.0           0.0    0.0    0.0   0.0    0.0  ...      0.0   \n",
       "4           0.0           0.0    0.0    0.0   0.0    0.0  ...      0.0   \n",
       "..          ...           ...    ...    ...   ...    ...  ...      ...   \n",
       "995         0.0           0.0    0.0    0.0   0.0    0.0  ...      0.0   \n",
       "996         0.0           0.0    0.0    0.0   0.0    0.0  ...      0.0   \n",
       "997         0.0           0.0    0.0    0.0   0.0    0.0  ...      0.0   \n",
       "998         0.0           0.0    0.0    0.0   0.0    0.0  ...      0.0   \n",
       "999         0.0           0.0    0.0    0.0   0.0    0.0  ...      0.0   \n",
       "\n",
       "     „ÇÜ„ÅÜ„Åó„ÇÉ„ÅÆË°£Ë£Ö  „Éé„Éº„Éà   ÂãáÊ∞ó   Ê≤≥Á´•  ÁßªÂä®‰∏≠ÂõΩÁâà  ÁÆÄ‰Ωì‰∏≠Êñá  ùêºùëöùëùùëíùëõùëëùëñùëõùëî  ùëéùëùùëùùëüùëúùëéùëê‚Ñéùëíùë†  ùëëùëúùëúùëö  \n",
       "0        0.0  0.0  0.0  0.0    0.0   0.0        0.0         0.0   0.0  \n",
       "1        0.0  0.0  0.0  0.0    0.0   0.0        0.0         0.0   0.0  \n",
       "2        0.0  0.0  0.0  0.0    0.0   0.0        0.0         0.0   0.0  \n",
       "3        0.0  0.0  0.0  0.0    0.0   0.0        0.0         0.0   0.0  \n",
       "4        0.0  0.0  0.0  0.0    0.0   0.0        0.0         0.0   0.0  \n",
       "..       ...  ...  ...  ...    ...   ...        ...         ...   ...  \n",
       "995      0.0  0.0  0.0  0.0    0.0   0.0        0.0         0.0   0.0  \n",
       "996      0.0  0.0  0.0  0.0    0.0   0.0        0.0         0.0   0.0  \n",
       "997      0.0  0.0  0.0  0.0    0.0   0.0        0.0         0.0   0.0  \n",
       "998      0.0  0.0  0.0  0.0    0.0   0.0        0.0         0.0   0.0  \n",
       "999      0.0  0.0  0.0  0.0    0.0   0.0        0.0         0.0   0.0  \n",
       "\n",
       "[976 rows x 32303 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer=TfidfVectorizer(use_idf=True, smooth_idf=False)\n",
    "df = pd.read_csv('processed_text.csv', index_col=0).fillna(' ')\n",
    "df_vectorized = pd.DataFrame(vectorizer.fit_transform(df['lemma']).toarray(), index=df.index, columns=vectorizer.get_feature_names_out())\n",
    "df_vectorized = pd.concat((df[['title', 'link']], df_vectorized), axis=1)\n",
    "df_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8654a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>The Destroyer</td>\n",
       "      <td>/wiki/The_Destroyer</td>\n",
       "      <td>0.839928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Wall of Flesh</td>\n",
       "      <td>/wiki/Wall_of_Flesh</td>\n",
       "      <td>0.802796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Queen Bee</td>\n",
       "      <td>/wiki/Queen_Bee</td>\n",
       "      <td>0.791660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Skeletron</td>\n",
       "      <td>/wiki/Skeletron</td>\n",
       "      <td>0.788674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Frost Legion</td>\n",
       "      <td>/wiki/Frost_Legion</td>\n",
       "      <td>0.788131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Deerclops</td>\n",
       "      <td>/wiki/Deerclops</td>\n",
       "      <td>0.779030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Eye of Cthulhu</td>\n",
       "      <td>/wiki/Eye_of_Cthulhu</td>\n",
       "      <td>0.774815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Old Man</td>\n",
       "      <td>/wiki/Old_Man</td>\n",
       "      <td>0.764005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>Fallen Star</td>\n",
       "      <td>/wiki/Fallen_Star</td>\n",
       "      <td>0.762149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Arms Dealer</td>\n",
       "      <td>/wiki/Arms_Dealer</td>\n",
       "      <td>0.762005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              title                  link  similarity\n",
       "138   The Destroyer   /wiki/The_Destroyer    0.839928\n",
       "135   Wall of Flesh   /wiki/Wall_of_Flesh    0.802796\n",
       "132       Queen Bee       /wiki/Queen_Bee    0.791660\n",
       "134       Skeletron       /wiki/Skeletron    0.788674\n",
       "155    Frost Legion    /wiki/Frost_Legion    0.788131\n",
       "133       Deerclops       /wiki/Deerclops    0.779030\n",
       "129  Eye of Cthulhu  /wiki/Eye_of_Cthulhu    0.774815\n",
       "111         Old Man         /wiki/Old_Man    0.764005\n",
       "621     Fallen Star     /wiki/Fallen_Star    0.762149\n",
       "100     Arms Dealer     /wiki/Arms_Dealer    0.762005"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def page_hist_lemma(history):\n",
    "    lemma = []\n",
    "    for page in history:\n",
    "        lemma = df[df['title'] == page]['lemma']\n",
    "    return lemma\n",
    "\n",
    "def get_recommendations(df, df_vectorized, history):\n",
    "    query_vec = vectorizer.transform(page_hist_lemma(history))\n",
    "    # cosine similarity sometimes can't find proper indices\n",
    "    df = df.reset_index(drop=True)\n",
    "    df_vectorized = df_vectorized.reset_index(drop=True)\n",
    "    similarities = cosine_similarity(df_vectorized.iloc[:,2:], query_vec).ravel()\n",
    "\n",
    "    # prevents getting recommended the same articles again\n",
    "    similarities[df['title'].isin(history)] = -float('inf')\n",
    "    top_indicies = similarities.argsort()[::-1][:10]\n",
    "    res = df.loc[top_indicies, ['title', 'link']].copy()\n",
    "    res['similarity'] = similarities[top_indicies]\n",
    "    return res.sort_values(by='similarity', ascending=False)\n",
    "\n",
    "history = [\n",
    "    'Brain of Cthulu',\n",
    "    'Eater of Worlds'\n",
    "]\n",
    "\n",
    "get_recommendations(df, df_vectorized, history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
